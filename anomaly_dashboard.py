import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime, timedelta
from collections import defaultdict
import os
import glob

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="An√°lise de Anomalias - Pageviews",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personalizado
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        border-left: 4px solid #1f77b4;
    }
    .anomaly-critical {
        background-color: #ffebee;
        border-left: 4px solid #f44336;
    }
    .anomaly-warning {
        background-color: #fff3e0;
        border-left: 4px solid #ff9800;
    }
    .anomaly-info {
        background-color: #e3f2fd;
        border-left: 4px solid #2196f3;
    }
</style>
""", unsafe_allow_html=True)

def load_and_prepare_data(uploaded_file):
    """Carrega e prepara os dados do CSV"""
    try:
        # Ler o CSV
        df = pd.read_csv(uploaded_file)
        
        # Verificar se as colunas necess√°rias existem
        if 'Date + hour (YYYYMMDDHH)' in df.columns and 'Views' in df.columns:
            # Formato novo: "Date + hour (YYYYMMDDHH)", "Views"
            date_hour_col = 'Date + hour (YYYYMMDDHH)'
            
            # Extrair data e hora da string
            df['Date'] = pd.to_datetime(df[date_hour_col].str.extract(r'(\w+ \d+, \d+)')[0], format='%b %d, %Y')
            
            # Extrair hora da string (formato "12AM", "1PM", etc.)
            hour_str = df[date_hour_col].str.extract(r'(\d+)(AM|PM)')
            hour_num = hour_str[0].astype(int)
            am_pm = hour_str[1]
            
            # Converter para formato 24h
            df['Hour'] = hour_num.where(am_pm == 'AM', hour_num + 12)
            df['Hour'] = df['Hour'].where(df['Hour'] != 12, 0)  # 12AM = 0h
            df['Hour'] = df['Hour'].where(df['Hour'] != 24, 12)  # 12PM = 12h
            
        elif 'Date' in df.columns and 'Hour' in df.columns and 'Views' in df.columns:
            # Formato antigo: "Date", "Hour", "Views"
            df['Date'] = pd.to_datetime(df['Date'], format='%b %d, %Y', errors='coerce')
            
        else:
            st.error("‚ùå O CSV deve conter as colunas: 'Date + hour (YYYYMMDDHH)' e 'Views' OU 'Date', 'Hour' e 'Views'")
            return None
        
        # Verificar se a convers√£o foi bem-sucedida
        if df['Date'].isna().any():
            st.error("‚ùå Erro ao converter datas. Verifique o formato das datas no CSV.")
            return None
        
        # Criar coluna de data e hora combinada
        df['DateTime'] = df['Date'] + pd.to_timedelta(df['Hour'], unit='h')
        
        # Ordenar por data e hora
        df = df.sort_values(['Date', 'Hour']).reset_index(drop=True)
        
        return df
        
    except Exception as e:
        st.error(f"‚ùå Erro ao processar o arquivo: {str(e)}")
        return None

def calculate_statistics(df):
    """Calcula estat√≠sticas b√°sicas dos dados"""
    views = df['Views'].values
    
    # Verificar se h√° dados
    if len(views) == 0:
        return {
            'total': 0,
            'count': 0,
            'mean': 0.0,
            'median': 0.0,
            'std': 0.0,
            'min': 0,
            'max': 0,
            'q1': 0.0,
            'q3': 0.0,
            'iqr': 0.0,
            'lower_bound': 0.0,
            'upper_bound': 0.0
        }
    
    stats = {
        'total': int(np.sum(views)),
        'count': len(views),
        'mean': float(np.mean(views)),
        'median': float(np.median(views)),
        'std': float(np.std(views)),
        'min': int(np.min(views)),
        'max': int(np.max(views))
    }
    
    # Calcular quartis
    q1 = np.percentile(views, 25)
    q3 = np.percentile(views, 75)
    iqr = q3 - q1
    
    stats['q1'] = float(q1)
    stats['q3'] = float(q3)
    stats['iqr'] = float(iqr)
    stats['lower_bound'] = float(q1 - 1.5 * iqr)
    stats['upper_bound'] = float(q3 + 1.5 * iqr)
    
    return stats

def calculate_hourly_means(df):
    """Calcula a m√©dia hist√≥rica para cada hora do dia"""
    hourly_means = df.groupby('Hour')['Views'].mean().to_dict()
    return hourly_means

def detect_low_values_by_hour(df, hourly_means, threshold_percentage=0.5):
    """
    Detecta valores baixos comparando com a m√©dia hist√≥rica de cada hora
    threshold_percentage: percentual da m√©dia hist√≥rica (ex: 0.5 = 50% da m√©dia)
    """
    low_values = []
    
    for _, row in df.iterrows():
        hour = row['Hour']
        current_value = row['Views']
        historical_mean = hourly_means.get(hour, 0)
        
        # Se a m√©dia hist√≥rica for muito baixa (menos que 5), n√£o considerar como anomalia
        if historical_mean < 5:
            continue
            
        # Se o valor atual for menor que X% da m√©dia hist√≥rica dessa hora
        if current_value < (historical_mean * threshold_percentage):
            low_values.append(row)
    
    return pd.DataFrame(low_values) if low_values else pd.DataFrame()

def detect_anomalies(df, stats, threshold_percentage=0.5, extreme_threshold=0.8, start_hour=0, end_hour=23):
    """Detecta diferentes tipos de anomalias"""
    # Verificar se h√° dados
    if len(df) == 0:
        return {
            'zero_views': pd.DataFrame(),
            'very_low_views': pd.DataFrame(),
            'statistical_outliers': pd.DataFrame(),
            'extreme_variations': pd.DataFrame(),
            'missing_hours': []
        }
    
    # Filtrar dados pelo range de horas
    df_filtered = df[(df['Hour'] >= start_hour) & (df['Hour'] <= end_hour)]
    
    # Calcular m√©dias hist√≥ricas por hora
    hourly_means = calculate_hourly_means(df)
    
    # Detectar valores baixos comparando com a m√©dia hist√≥rica de cada hora
    very_low_views = detect_low_values_by_hour(df_filtered, hourly_means, threshold_percentage)
    
    anomalies = {
        'zero_views': df_filtered[df_filtered['Views'] == 0],
        'very_low_views': very_low_views,
        'statistical_outliers': df_filtered[
            (df_filtered['Views'] < stats['lower_bound']) | 
            (df_filtered['Views'] > stats['upper_bound'])
        ],
        'extreme_variations': df_filtered[
            (abs(df_filtered['Views'] - stats['mean']) / stats['mean'] > extreme_threshold) &
            ((df_filtered['Views'] > stats['mean'] * 2) | (df_filtered['Views'] < stats['mean'] * 0.3))
        ]
    }
    
    # Verificar horas faltando (apenas no range selecionado)
    expected_hours = set(range(start_hour, end_hour + 1))  # Range selecionado
    daily_hours = df.groupby('Date')['Hour'].apply(set).to_dict()
    missing_hours = []
    
    for date, hours in daily_hours.items():
        missing = expected_hours - hours
        if missing:
            missing_hours.append({
                'date': date,
                'missing_hours': sorted(list(missing))
            })
    
    anomalies['missing_hours'] = missing_hours
    
    return anomalies

def create_time_series_chart(df, anomalies, stats):
    """Cria gr√°fico de s√©rie temporal com anomalias destacadas"""
    fig = go.Figure()
    
    # Linha principal
    fig.add_trace(go.Scatter(
        x=df['DateTime'],
        y=df['Views'],
        mode='lines',
        name='Pageviews',
        line=dict(color='steelblue', width=1),
        opacity=0.7
    ))
    
    # Destacar anomalias
    if not anomalies['zero_views'].empty:
        fig.add_trace(go.Scatter(
            x=anomalies['zero_views']['DateTime'],
            y=anomalies['zero_views']['Views'],
            mode='markers',
            name='Valores Zero',
            marker=dict(color='red', size=8, symbol='x'),
            hovertemplate='<b>%{x}</b><br>Views: %{y}<extra></extra>'
        ))
    
    if not anomalies['very_low_views'].empty:
        fig.add_trace(go.Scatter(
            x=anomalies['very_low_views']['DateTime'],
            y=anomalies['very_low_views']['Views'],
            mode='markers',
            name='Valores Muito Baixos',
            marker=dict(color='orange', size=6),
            hovertemplate='<b>%{x}</b><br>Views: %{y}<extra></extra>'
        ))
    
    if not anomalies['statistical_outliers'].empty:
        fig.add_trace(go.Scatter(
            x=anomalies['statistical_outliers']['DateTime'],
            y=anomalies['statistical_outliers']['Views'],
            mode='markers',
            name='Outliers Estat√≠sticos',
            marker=dict(color='purple', size=6),
            hovertemplate='<b>%{x}</b><br>Views: %{y}<extra></extra>'
        ))
    
    # Linha da m√©dia
    fig.add_hline(
        y=stats['mean'],
        line_dash="dash",
        line_color="green",
        annotation_text=f"M√©dia: {stats['mean']:.0f}",
        annotation_position="top right"
    )
    
    fig.update_layout(
        title='S√©rie Temporal de Pageviews com Anomalias Destacadas',
        xaxis_title='Data e Hora',
        yaxis_title='Pageviews',
        hovermode='x unified',
        height=500
    )
    
    return fig


def create_hourly_analysis_chart(df):
    """Cria an√°lise por hora do dia"""
    hourly_stats = df.groupby('Hour')['Views'].agg(['mean', 'std', 'count', 'min', 'max']).reset_index()
    
    fig = go.Figure()
    
    # Gr√°fico de barras com barras de erro
    fig.add_trace(go.Bar(
        x=hourly_stats['Hour'],
        y=hourly_stats['mean'],
        name='M√©dia por Hora',
        marker_color='steelblue',
        error_y=dict(
            type='data', 
            array=hourly_stats['std'], 
            visible=True,
            color='rgba(0,0,0,0.3)'
        ),
        hovertemplate='<b>Hora:</b> %{x}h<br><b>M√©dia:</b> %{y:.0f}<br><b>Desvio:</b> ¬±%{error_y.array:.0f}<extra></extra>'
    ))
    
    # Adicionar linha de tend√™ncia
    fig.add_trace(go.Scatter(
        x=hourly_stats['Hour'],
        y=hourly_stats['mean'],
        mode='lines+markers',
        name='Tend√™ncia',
        line=dict(color='red', width=2),
        marker=dict(size=6),
        hovertemplate='<b>Hora:</b> %{x}h<br><b>M√©dia:</b> %{y:.0f}<extra></extra>'
    ))
    
    fig.update_layout(
        title='M√©dia de Pageviews por Hora do Dia',
        xaxis_title='Hora do Dia',
        yaxis_title='Pageviews',
        height=500,
        hovermode='x unified',
        xaxis=dict(tickmode='linear', tick0=0, dtick=1, range=[-0.5, 23.5])
    )
    
    return fig

def create_anomaly_summary_chart(anomalies):
    """Cria gr√°fico resumo das anomalias"""
    anomaly_counts = {
        'Valores Zero': len(anomalies['zero_views']),
        'Valores Muito Baixos': len(anomalies['very_low_views']),
        'Outliers Estat√≠sticos': len(anomalies['statistical_outliers']),
        'Varia√ß√µes Extremas': len(anomalies['extreme_variations']),
        'Dias com Horas Faltando': len(anomalies['missing_hours'])
    }
    
    fig = go.Figure(data=[
        go.Bar(
            x=list(anomaly_counts.keys()),
            y=list(anomaly_counts.values()),
            marker_color=['red', 'orange', 'purple', 'blue', 'gray']
        )
    ])
    
    fig.update_layout(
        title='Resumo de Anomalias Detectadas',
        xaxis_title='Tipo de Anomalia',
        yaxis_title='Quantidade',
        height=400
    )
    
    return fig

def generate_report_text(df, stats, anomalies):
    """Gera texto do relat√≥rio"""
    report_lines = []
    report_lines.append("RELAT√ìRIO DE AN√ÅLISE DE ANOMALIAS - PAGEVIEWS")
    report_lines.append("=" * 60)
    report_lines.append(f"Per√≠odo: {df['Date'].min().strftime('%d/%m/%Y')} a {df['Date'].max().strftime('%d/%m/%Y')}")
    report_lines.append(f"Total de registros: {len(df):,}")
    report_lines.append("")
    
    report_lines.append("ESTAT√çSTICAS GERAIS:")
    report_lines.append(f"‚Ä¢ Total de pageviews: {stats['total']:,}")
    report_lines.append(f"‚Ä¢ M√©dia por hora: {stats['mean']:.1f}")
    report_lines.append(f"‚Ä¢ Mediana: {stats['median']:.1f}")
    report_lines.append(f"‚Ä¢ Desvio padr√£o: {stats['std']:.1f}")
    report_lines.append("")
    
    report_lines.append("ANOMALIAS DETECTADAS:")
    report_lines.append(f"‚Ä¢ Valores zero: {len(anomalies['zero_views'])} registros")
    report_lines.append(f"‚Ä¢ Valores muito baixos: {len(anomalies['very_low_views'])} registros")
    report_lines.append(f"‚Ä¢ Outliers estat√≠sticos: {len(anomalies['statistical_outliers'])} registros")
    report_lines.append(f"‚Ä¢ Varia√ß√µes extremas: {len(anomalies['extreme_variations'])} registros")
    report_lines.append(f"‚Ä¢ Dias com horas faltando: {len(anomalies['missing_hours'])} dias")
    report_lines.append("")
    
    # An√°lise de cobertura por hora
    all_hours = set(range(24))
    hours_with_data = set(df['Hour'].unique())
    missing_hours = all_hours - hours_with_data
    
    # Calcular cobertura real considerando dias com horas faltando
    total_expected_hours = len(df['Date'].unique()) * 24
    total_actual_hours = len(df)
    coverage_percentage = (total_actual_hours / total_expected_hours) * 100 if total_expected_hours > 0 else 100
    
    report_lines.append("COBERTURA POR HORA:")
    report_lines.append(f"‚Ä¢ Horas com dados: {total_actual_hours:,}/{total_expected_hours:,} ({coverage_percentage:.1f}%)")
    if missing_hours:
        report_lines.append(f"‚Ä¢ Horas sem dados: {sorted(missing_hours)}")
    else:
        report_lines.append("‚Ä¢ Todas as 24 horas possuem dados")
    
    # Informa√ß√£o adicional sobre dias com problemas
    if anomalies['missing_hours']:
        total_missing_hours = sum(len(day['missing_hours']) for day in anomalies['missing_hours'])
        report_lines.append(f"‚Ä¢ Total de horas faltando: {total_missing_hours}")
    
    return "\n".join(report_lines)

@st.cache_data
def load_multiple_datasets(uploaded_files):
    """Carrega m√∫ltiplos datasets e retorna um dicion√°rio com os dados"""
    datasets = {}
    
    for uploaded_file in uploaded_files:
        # Extrair nome da marca do nome do arquivo
        brand_name = uploaded_file.name.replace('.csv', '').replace('_', ' ').title()
        
        # Carregar dados
        df = load_and_prepare_data(uploaded_file)
        if df is not None:
            datasets[brand_name] = df
    
    return datasets

@st.cache_data
def load_csv_files_from_directory():
    """Carrega automaticamente todos os arquivos CSV da pasta csvs/"""
    datasets = {}
    
    # Buscar todos os arquivos CSV na pasta csvs/
    csv_files = glob.glob("csvs/*.csv")
    
    for csv_file in csv_files:
        try:
            # Extrair nome da marca do nome do arquivo
            brand_name = os.path.basename(csv_file).replace('.csv', '').replace('_', ' ').title()
            
            # Carregar dados usando pandas diretamente
            df = pd.read_csv(csv_file)
            
            # Verificar se as colunas necess√°rias existem
            if 'Date + hour (YYYYMMDDHH)' in df.columns and 'Views' in df.columns:
                # Formato novo: "Date + hour (YYYYMMDDHH)", "Views"
                date_hour_col = 'Date + hour (YYYYMMDDHH)'
                
                # Extrair data e hora da string
                df['Date'] = pd.to_datetime(df[date_hour_col].str.extract(r'(\w+ \d+, \d+)')[0], format='%b %d, %Y')
                
                # Extrair hora da string (formato "12AM", "1PM", etc.)
                hour_str = df[date_hour_col].str.extract(r'(\d+)(AM|PM)')
                hour_num = hour_str[0].astype(int)
                am_pm = hour_str[1]
                
                # Converter para formato 24h
                df['Hour'] = hour_num.where(am_pm == 'AM', hour_num + 12)
                df['Hour'] = df['Hour'].where(df['Hour'] != 12, 0)  # 12AM = 0h
                df['Hour'] = df['Hour'].where(df['Hour'] != 24, 12)  # 12PM = 12h
                
            elif 'Date' in df.columns and 'Hour' in df.columns and 'Views' in df.columns:
                # Formato antigo: "Date", "Hour", "Views"
                df['Date'] = pd.to_datetime(df['Date'], format='%b %d, %Y', errors='coerce')
                
            else:
                continue  # Pular arquivos que n√£o t√™m o formato correto
            
            # Verificar se a convers√£o foi bem-sucedida
            if df['Date'].isna().any():
                continue  # Pular se h√° problemas na convers√£o de datas
            
            # Criar coluna de data e hora combinada
            df['DateTime'] = df['Date'] + pd.to_timedelta(df['Hour'], unit='h')
            
            # Ordenar por data e hora
            df = df.sort_values(['Date', 'Hour']).reset_index(drop=True)
            
            datasets[brand_name] = df
            
        except Exception as e:
            continue  # Pular arquivos com erro
    
    return datasets

def create_comparison_chart(datasets, stats_dict, selected_brands=None, aggregation='hourly'):
    """Cria gr√°fico de compara√ß√£o entre marcas selecionadas
    
    Args:
        datasets: Dicion√°rio com datasets das marcas
        stats_dict: Dicion√°rio com estat√≠sticas das marcas
        selected_brands: Lista de marcas selecionadas
        aggregation: 'hourly' para dados por hora, 'daily' para dados por dia
    """
    fig = go.Figure()
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
    
    # Se n√£o especificou marcas, usar todas
    if selected_brands is None:
        selected_brands = list(datasets.keys())
    
    for i, brand_name in enumerate(selected_brands):
        if brand_name in datasets:
            df = datasets[brand_name]
            color = colors[i % len(colors)]
            
            if aggregation == 'hourly':
                # Usar dados por hora (DateTime)
                fig.add_trace(go.Scatter(
                    x=df['DateTime'],
                    y=df['Views'],
                    mode='lines+markers',
                    name=brand_name,
                    line=dict(color=color, width=1.5),
                    marker=dict(size=3),
                    hovertemplate=f'<b>{brand_name}</b><br>' +
                                  'Data: %{x|%d/%m/%Y}<br>' +
                                  'Hora: %{x|%H}h<br>' +
                                  'Views: %{y:,}<extra></extra>'
                ))
            else:
                # Agrupar por data para ter um valor por dia
                daily_views = df.groupby('Date')['Views'].sum().reset_index()
                
                fig.add_trace(go.Scatter(
                    x=daily_views['Date'],
                    y=daily_views['Views'],
                    mode='lines+markers',
                    name=brand_name,
                    line=dict(color=color, width=2),
                    marker=dict(size=4),
                    hovertemplate=f'<b>{brand_name}</b><br>' +
                                  'Data: %{x|%d/%m/%Y}<br>' +
                                  'Views: %{y:,}<extra></extra>'
                ))
    
    title_suffix = "por Hora" if aggregation == 'hourly' else "por Dia"
    fig.update_layout(
        title=f'Compara√ß√£o de Pageviews entre Marcas Selecionadas - {title_suffix}',
        xaxis_title='Data e Hora' if aggregation == 'hourly' else 'Data',
        yaxis_title='Pageviews',
        height=500,
        hovermode='x unified',
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        )
    )
    
    return fig

def create_comparison_metrics(datasets, stats_dict):
    """Cria m√©tricas de compara√ß√£o entre marcas"""
    comparison_data = []
    
    for brand_name, stats in stats_dict.items():
        comparison_data.append({
            'Marca': brand_name,
            'Total Pageviews': stats['total'],
            'Total Pageviews Formatado': f"{stats['total']:,}",
            'M√©dia por Hora': stats['mean'],
            'M√©dia por Hora Formatada': f"{stats['mean']:.1f}",
            'Mediana': f"{stats['median']:.1f}",
            'Desvio Padr√£o': f"{stats['std']:.1f}",
            'M√°ximo': f"{stats['max']:,}",
            'M√≠nimo': f"{stats['min']:,}"
        })
    
    return pd.DataFrame(comparison_data)

def create_anomaly_analysis_chart(anomaly_comparison_data):
    """Cria gr√°fico visual para an√°lise de anomalias por marca"""
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('Total de Anomalias', 'Anomalias Cr√≠ticas', 'Outliers Estat√≠sticos', 'Horas Faltando'),
        specs=[[{"type": "bar"}, {"type": "bar"}],
               [{"type": "bar"}, {"type": "bar"}]]
    )
    
    brands = [item['Marca'] for item in anomaly_comparison_data]
    
    # Total de anomalias
    total_anomalies = [item['Total Anomalias'] for item in anomaly_comparison_data]
    fig.add_trace(go.Bar(x=brands, y=total_anomalies, name='Total', marker_color='lightcoral'), row=1, col=1)
    
    # Anomalias cr√≠ticas (zero + baixos)
    critical_anomalies = [item['Valores Zero'] + item['Valores Baixos'] for item in anomaly_comparison_data]
    fig.add_trace(go.Bar(x=brands, y=critical_anomalies, name='Cr√≠ticas', marker_color='red'), row=1, col=2)
    
    # Outliers estat√≠sticos
    outliers = [item['Outliers'] for item in anomaly_comparison_data]
    fig.add_trace(go.Bar(x=brands, y=outliers, name='Outliers', marker_color='orange'), row=2, col=1)
    
    # Horas faltando
    missing_hours = [item['Horas Faltando'] for item in anomaly_comparison_data]
    fig.add_trace(go.Bar(x=brands, y=missing_hours, name='Horas Faltando', marker_color='gray'), row=2, col=2)
    
    fig.update_layout(height=600, showlegend=False, title_text="An√°lise Visual de Anomalias por Marca")
    return fig

def calculate_anomaly_severity_score(anomaly_data):
    """Calcula um score de severidade das anomalias (0-100)"""
    # Pesos para diferentes tipos de anomalias
    weights = {
        'zero_views': 10,      # Muito cr√≠tico
        'very_low_views': 5,   # Cr√≠tico
        'statistical_outliers': 2,  # Moderado
        'extreme_variations': 3,    # Moderado-alto
        'missing_hours': 1     # Baixo
    }
    
    total_records = anomaly_data.get('total_records', 1)
    
    # Calcular score ponderado
    score = (
        len(anomaly_data['zero_views']) * weights['zero_views'] +
        len(anomaly_data['very_low_views']) * weights['very_low_views'] +
        len(anomaly_data['statistical_outliers']) * weights['statistical_outliers'] +
        len(anomaly_data['extreme_variations']) * weights['extreme_variations'] +
        len(anomaly_data['missing_hours']) * weights['missing_hours']
    )
    
    # Se n√£o h√° anomalias, retornar 0
    if score == 0:
        return 0
    
    # Normalizar por total de registros (usar uma base mais conservadora)
    # Assumir que 1% de anomalias cr√≠ticas = score 50
    normalized_score = min(100, (score / total_records) * 5000)
    
    return normalized_score

def get_anomaly_status(score):
    """Retorna o status baseado no score de severidade"""
    if score >= 50:
        return "üî¥ Cr√≠tico"
    elif score >= 25:
        return "üü° Aten√ß√£o"
    elif score >= 10:
        return "üü† Moderado"
    elif score > 0:
        return "üü¢ Baixo"
    else:
        return "‚úÖ Saud√°vel"

def check_password():
    """Verifica se a senha est√° correta"""
    def password_entered():
        """Verifica se a senha inserida est√° correta"""
        if "password" in st.session_state:
            if st.session_state["password"] == st.secrets.get("password", "admin123"):
                st.session_state["password_correct"] = True
                del st.session_state["password"]  # Remove a senha da sess√£o por seguran√ßa
            else:
                st.session_state["password_correct"] = False

    # Retorna True se a senha estiver correta
    if st.session_state.get("password_correct", False):
        return True

    # Tela de login
    st.markdown("""
    <div style="text-align: center; padding: 2rem;">
        <h1 style="color: #1f77b4; margin-bottom: 2rem;">üîê Acesso Restrito</h1>
        <p style="font-size: 1.2rem; color: #666;">Digite a senha para acessar o dashboard</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Input da senha
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        password = st.text_input(
            "Senha:",
            type="password",
            key="password_input",
            help="Digite a senha para acessar o dashboard"
        )
        
        if st.button("üöÄ ENTRAR", type="primary", use_container_width=True):
            if password:
                st.session_state["password"] = password
                password_entered()
                st.rerun()  # For√ßa o rerun para aplicar a mudan√ßa
            else:
                st.error("‚ùå Por favor, digite a senha!")
    
    if "password_correct" in st.session_state:
        if not st.session_state["password_correct"]:
            st.error("‚ùå Senha incorreta. Tente novamente.")
    
    return False

def main():
    # Cabe√ßalho principal
    st.markdown('<h1 class="main-header">üìä An√°lise de Anomalias - Pageviews</h1>', unsafe_allow_html=True)
    
    # Carregar CSVs automaticamente da pasta (com cache)
    datasets = load_csv_files_from_directory()

    if datasets:
        st.sidebar.subheader("Marcas Carregadas")
        
        # Definir BYD como padr√£o se dispon√≠vel
        available_brands = list(datasets.keys())
        default_index = 0
        if "Byd" in available_brands:
            default_index = available_brands.index("Byd")
        
        selected_brand = st.sidebar.selectbox(
            "Selecione a marca para an√°lise:",
            options=available_brands,
            index=default_index,
        )
        # Informa√ß√µes da marca selecionada
        selected_df = datasets[selected_brand]
        selected_stats = calculate_statistics(selected_df)
        
        st.sidebar.metric("Total Views", f"{selected_stats['total']:,}")
        st.sidebar.metric("M√©dia/Hora", f"{selected_stats['mean']:.0f}")
        
        # Dataset selecionado
        df = datasets[selected_brand]
        
        # Date picker geral para filtrar todos os dados
        st.sidebar.subheader("üìÖ Filtro de Per√≠odo Geral")
        st.sidebar.caption("Filtra todos os dados para o per√≠odo selecionado")
        
        col1, col2 = st.sidebar.columns(2)
        with col1:
            start_date_global = st.date_input(
                "Data Inicial",
                value=st.session_state.get('global_start_date', df['Date'].min().date()),
                min_value=df['Date'].min().date(),
                max_value=df['Date'].max().date(),
                help="Data inicial para an√°lise geral",
                key="global_start_date"
            )
        
        with col2:
            end_date_global = st.date_input(
                "Data Final",
                value=st.session_state.get('global_end_date', df['Date'].max().date()),
                min_value=df['Date'].min().date(),
                max_value=df['Date'].max().date(),
                help="Data final para an√°lise geral",
                key="global_end_date"
            )
        
        # Validar se a data final √© maior que a inicial
        if end_date_global < start_date_global:
            st.sidebar.error("‚ö†Ô∏è **Erro:** Data final deve ser maior que a data inicial!")
            st.sidebar.caption("Ajuste as datas para continuar a an√°lise")
            # Usar dados originais se as datas estiverem incorretas
            df = df
        else:
            # Aplicar filtro de data global
            start_datetime_global = pd.to_datetime(start_date_global)
            end_datetime_global = pd.to_datetime(end_date_global) + pd.Timedelta(days=1)
            
            df_filtered_global = df[(df['Date'] >= start_datetime_global) & (df['Date'] < end_datetime_global)]
            
            # Verificar se h√° dados ap√≥s o filtro
            if len(df_filtered_global) == 0:
                st.sidebar.warning("‚ö†Ô∏è **Aten√ß√£o:** Nenhum dado encontrado para o per√≠odo selecionado!")
                st.sidebar.caption("Usando todos os dados dispon√≠veis")
                df = df
        
        # Configura√ß√µes de detec√ß√£o
        st.sidebar.subheader("üîç Par√¢metros de Detec√ß√£o")
        
        # Range de horas
        st.sidebar.markdown("**‚è∞ Range de Horas**")
        col1, col2 = st.sidebar.columns(2)
        with col1:
            start_hour = st.selectbox(
                "Hora inicial",
                options=list(range(0, 24)),
                index=0,
                format_func=lambda x: f"{x:02d}h",
                help="Hora inicial para an√°lise de anomalias"
            )
        with col2:
            end_hour = st.selectbox(
                "Hora final",
                options=list(range(0, 24)),
                index=23,
                format_func=lambda x: f"{x:02d}h",
                help="Hora final para an√°lise de anomalias"
            )
        
        # Ajustar end_hour se for menor que start_hour
        if end_hour <= start_hour:
            end_hour = start_hour
            st.sidebar.warning("‚ö†Ô∏è Hora final ajustada para ser maior que a inicial")
        
        extreme_threshold = st.sidebar.slider(
            "Limite para varia√ß√µes extremas (%)",
            min_value=50,
            max_value=200,
            value=80,
            help="Varia√ß√µes acima deste percentual da m√©dia ser√£o consideradas extremas"
        ) / 100
        
        
        # Calcular estat√≠sticas
        stats = calculate_statistics(df)
        
        # Calcular m√©dias hist√≥ricas por hora
        hourly_means = calculate_hourly_means(df)
        
        st.sidebar.caption("Valores baixos s√£o detectados comparando com a m√©dia hist√≥rica de cada hora espec√≠fica")
        
        # Configurar threshold de percentual da m√©dia hist√≥rica
        threshold_percentage = st.sidebar.slider(
            "Limite para valores baixos (% da m√©dia hist√≥rica da hora)",
            min_value=10,
            max_value=80,
            value=50,
            help="Valores abaixo deste percentual da m√©dia hist√≥rica da hora ser√£o considerados baixos"
        ) / 100
        # Resumo das marcas
        with st.sidebar.expander("üìã Detalhes das marcas", expanded=False):
            for brand_name, brand_df in datasets.items():
                total_views = brand_df['Views'].sum()
                date_range = f"{brand_df['Date'].min().strftime('%d/%m/%Y')} a {brand_df['Date'].max().strftime('%d/%m/%Y')}"
                total_records = len(brand_df)
                avg_views = brand_df['Views'].mean()
                
                st.markdown(f"""
                **üè¢ {brand_name}**
                - üìà **{total_views:,}** views totais
                - üìÖ **{date_range}**
                - üìä **{total_records:,}** registros
                - üìâ **{avg_views:.0f}** views/hora (m√©dia)
                """)
                st.markdown("---")
        
    
        # Detectar anomalias
        anomalies = detect_anomalies(df, stats, threshold_percentage, extreme_threshold, start_hour, end_hour)
        
        # M√©tricas principais
        st.subheader("üìà M√©tricas Principais")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                label="Total de Pageviews",
                value=f"{stats['total']:,}",
                delta=None
            )
        
        with col2:
            st.metric(
                label="M√©dia por Hora",
                value=f"{stats['mean']:.0f}",
                delta=None
            )
        
        with col3:
            total_anomalies = (len(anomalies['zero_views']) + 
                             len(anomalies['very_low_views']) + 
                             len(anomalies['statistical_outliers']) + 
                             len(anomalies['extreme_variations']) + 
                             len(anomalies['missing_hours']))
            st.metric(
                label="Total de Anomalias",
                value=total_anomalies,
                delta=None
            )
        
        with col4:
            critical_anomalies = len(anomalies['zero_views']) + len(anomalies['very_low_views'])
            st.metric(
                label="Anomalias Cr√≠ticas",
                value=critical_anomalies,
                delta=None
            )
        
        # Alertas
        if critical_anomalies > 0:
            st.error(f"‚ö†Ô∏è **ATEN√á√ÉO**: {critical_anomalies} anomalias cr√≠ticas detectadas!")
        elif total_anomalies > 20:
            st.warning(f"‚ö†Ô∏è {total_anomalies} anomalias detectadas. Verifique os dados.")
        else:
            st.success("‚úÖ Nenhuma instabilidade cr√≠tica detectada.")
        
        # Tabs para diferentes visualiza√ß√µes
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "üö® Anomalias", 
            "üìä S√©rie Temporal", 
            "‚è∞ An√°lise por Hora", 
            "üìã Relat√≥rio",
            "üîÑ Compara√ß√£o de Marcas"
        ])
        
        with tab1:
            st.subheader("üö® Detalhes das Anomalias")
            
            # Resumo das anomalias
            st.plotly_chart(
                create_anomaly_summary_chart(anomalies),
                use_container_width=True
            )
            
            # Detalhes espec√≠ficos - Valores Zero
            if not anomalies['zero_views'].empty:
                st.subheader("‚ùå Valores Zero")
                st.dataframe(
                    anomalies['zero_views'][['Date', 'Hour', 'Views']].style.format({
                        'Date': lambda x: x.strftime('%d/%m/%Y'),
                        'Views': '{:,.0f}'
                    }),
                    use_container_width=True
                )
            
            # Valores Muito Baixos
            if not anomalies['very_low_views'].empty:
                st.subheader("‚ö†Ô∏è Valores Muito Baixos")
                st.dataframe(
                    anomalies['very_low_views'][['Date', 'Hour', 'Views']].head(10).style.format({
                        'Date': lambda x: x.strftime('%d/%m/%Y'),
                        'Views': '{:,.0f}'
                    }),
                    use_container_width=True
                )
            
            # Outliers Estat√≠sticos
            if not anomalies['statistical_outliers'].empty:
                st.subheader("üìà Outliers Estat√≠sticos (Top 10)")
                outliers_display = anomalies['statistical_outliers'].copy()
                outliers_display['Variation_%'] = ((outliers_display['Views'] - stats['mean']) / stats['mean'] * 100).round(1)
                st.dataframe(
                    outliers_display[['Date', 'Hour', 'Views', 'Variation_%']].head(10).style.format({
                        'Date': lambda x: x.strftime('%d/%m/%Y'),
                        'Views': '{:,.0f}',
                        'Variation_%': '{:+.1f}%'
                    }),
                    use_container_width=True
                )
            
            # Varia√ß√µes extremas
            if not anomalies['extreme_variations'].empty:
                st.subheader("üìä Varia√ß√µes Extremas")
                extreme_display = anomalies['extreme_variations'].copy()
                extreme_display['Variation_%'] = ((extreme_display['Views'] - stats['mean']) / stats['mean'] * 100).round(1)
                st.dataframe(
                    extreme_display[['Date', 'Hour', 'Views', 'Variation_%']].style.format({
                        'Date': lambda x: x.strftime('%d/%m/%Y'),
                        'Views': '{:,.0f}',
                        'Variation_%': '{:+.1f}%'
                    }),
                    use_container_width=True
                )
            
            # Horas faltando
            if anomalies['missing_hours']:
                st.subheader("‚è∞ Horas Faltando")
                missing_df = pd.DataFrame(anomalies['missing_hours'])
                missing_df['missing_hours_str'] = missing_df['missing_hours'].apply(
                    lambda x: ', '.join([f"{h:02d}h" for h in x])
                )
                st.dataframe(
                    missing_df[['date', 'missing_hours_str']].style.format({
                        'date': lambda x: x.strftime('%d/%m/%Y')
                    }),
                    use_container_width=True
                )
        
        with tab2:
            st.plotly_chart(
                create_time_series_chart(df, anomalies, stats),
                use_container_width=True,
                key="time_series_chart"
            )
        
        with tab3:
            # Gr√°fico geral por hora (sem filtro)
            st.subheader("üìä An√°lise Geral por Hora (Todo o Per√≠odo)")
            st.plotly_chart(
                create_hourly_analysis_chart(df),
                use_container_width=True,
                key="hourly_analysis_general"
            )
            
            st.markdown("---")
            
            # Informa√ß√£o sobre o filtro global aplicado
            st.info(f"üìä **An√°lise por Hora** - Per√≠odo: {start_date_global} a {end_date_global}")
            st.caption("Use o filtro de per√≠odo geral na sidebar para ajustar o per√≠odo de an√°lise")
        
        with tab4:
            st.subheader("üìã Relat√≥rio Completo")
            
            # Gerar relat√≥rio
            report_text = generate_report_text(df, stats, anomalies)
            st.text_area("-", report_text, height=400)
            
            # Bot√£o de download
            st.download_button(
                label="üì• Baixar Relat√≥rio",
                data=report_text,
                file_name=f"anomaly_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain"
            )
            with tab5:
                st.subheader("üîÑ Compara√ß√£o entre Marcas")
                
                # Sele√ß√£o de marcas para compara√ß√£o
                st.subheader("üéØ Sele√ß√£o de Marcas")
                available_brands = list(datasets.keys())
                col1 = st.columns(1)[0]
                with col1:
                    selected_brands_for_comparison = st.multiselect(
                        "Selecione as marcas para comparar:",
                        options=available_brands,
                        default=available_brands[:3] if len(available_brands) >= 3 else available_brands,
                        help="Escolha quais marcas deseja comparar"
                    )
                
                if selected_brands_for_comparison:
                    # Aplicar filtro global tamb√©m na compara√ß√£o
                    selected_datasets = {}
                    for brand in selected_brands_for_comparison:
                        brand_df = datasets[brand]
                        # Aplicar o mesmo filtro de data global
                        brand_df_filtered = brand_df[(brand_df['Date'] >= start_datetime_global) & (brand_df['Date'] < end_datetime_global)]
                        selected_datasets[brand] = brand_df_filtered
                    
                    all_stats = {}
                    for brand_name, brand_df in selected_datasets.items():
                        all_stats[brand_name] = calculate_statistics(brand_df)
                    
                    # Gr√°fico de compara√ß√£o
                    st.subheader("üìà Evolu√ß√£o Temporal Comparativa")
                    
                    # Op√ß√£o para alternar entre visualiza√ß√£o di√°ria e hor√°ria
                    col1, col2 = st.columns([3, 1])
                    with col2:
                        aggregation_mode = st.radio(
                            "Granularidade:",
                            options=['hourly', 'daily'],
                            format_func=lambda x: "Por Hora" if x == 'hourly' else "Por Dia",
                            horizontal=True,
                            help="Escolha se deseja ver os dados por hora ou agregados por dia"
                        )
                    
                    st.plotly_chart(
                        create_comparison_chart(selected_datasets, all_stats, selected_brands_for_comparison, aggregation_mode),
                        use_container_width=True,
                        key="comparison_chart"
                    )
                
                    # M√©tricas comparativas
                    st.subheader("üìä M√©tricas Comparativas")
                    comparison_df = create_comparison_metrics(selected_datasets, all_stats)
                    
                    # Mostrar apenas colunas formatadas na tabela
                    display_df = comparison_df[['Marca', 'Total Pageviews Formatado', 'M√©dia por Hora Formatada', 'Mediana', 'Desvio Padr√£o', 'M√°ximo', 'M√≠nimo']]
                    display_df.columns = ['Marca', 'Total Pageviews', 'M√©dia por Hora', 'Mediana', 'Desvio Padr√£o', 'M√°ximo', 'M√≠nimo']
                    st.dataframe(display_df, use_container_width=True)
                
                    # Ranking de performance
                    st.subheader("üèÜ Ranking de Performance")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.markdown("**üìà Maior Volume Total**")
                        top_total = comparison_df.nlargest(1, 'Total Pageviews')[['Marca', 'Total Pageviews Formatado']]
                        top_total.columns = ['Marca', 'Total Pageviews']
                        st.dataframe(top_total, use_container_width=True)
                    
                    with col2:
                        st.markdown("**üìä Maior M√©dia por Hora**")
                        # Usar a coluna num√©rica original para ordena√ß√£o
                        top_avg_idx = comparison_df['M√©dia por Hora'].idxmax()
                        top_avg = comparison_df.loc[[top_avg_idx], ['Marca', 'M√©dia por Hora Formatada']]
                        top_avg.columns = ['Marca', 'M√©dia por Hora']
                        st.dataframe(top_avg, use_container_width=True)
                    
                    with col3:
                        st.markdown("**üìä Menor Desvio Padr√£o**")
                        # Usar a coluna num√©rica original para ordena√ß√£o
                        top_stable_idx = comparison_df['Desvio Padr√£o'].astype(float).idxmin()
                        top_stable = comparison_df.loc[[top_stable_idx], ['Marca', 'Desvio Padr√£o']]
                        st.dataframe(top_stable, use_container_width=True)
                
                    # An√°lise de anomalias comparativa
                    st.subheader("üö® An√°lise de Anomalias por Marca")
                    
                    anomaly_comparison = []
                    anomaly_details = {}
                    
                    for brand_name, brand_df in selected_datasets.items():
                        brand_stats = all_stats[brand_name]
                        # Usar threshold de 50% da m√©dia hist√≥rica para compara√ß√£o
                        brand_anomalies = detect_anomalies(brand_df, brand_stats, 0.5, 0.8, 0, 23)
                        
                        # Calcular score de severidade
                        anomaly_data_with_records = brand_anomalies.copy()
                        anomaly_data_with_records['total_records'] = len(brand_df)
                        severity_score = calculate_anomaly_severity_score(anomaly_data_with_records)
                        status = get_anomaly_status(severity_score)
                        
                        total_anomalies = (len(brand_anomalies['zero_views']) + 
                                         len(brand_anomalies['very_low_views']) + 
                                         len(brand_anomalies['statistical_outliers']) + 
                                         len(brand_anomalies['extreme_variations']) + 
                                         len(brand_anomalies['missing_hours']))
                        
                        critical_anomalies = len(brand_anomalies['zero_views']) + len(brand_anomalies['very_low_views'])
                        
                        anomaly_comparison.append({
                            'Marca': brand_name,
                            'Status': status,
                            'Score Severidade': f"{severity_score:.1f}",
                            'Total Anomalias': total_anomalies,
                            'Cr√≠ticas': critical_anomalies,
                            'Valores Zero': len(brand_anomalies['zero_views']),
                            'Valores Baixos': len(brand_anomalies['very_low_views']),
                            'Outliers': len(brand_anomalies['statistical_outliers']),
                            'Varia√ß√µes Extremas': len(brand_anomalies['extreme_variations']),
                            'Horas Faltando': len(brand_anomalies['missing_hours'])
                        })
                        
                        # Guardar detalhes para an√°lise posterior
                        anomaly_details[brand_name] = brand_anomalies
                    
                    # Ordenar por score de severidade (maior primeiro)
                    anomaly_comparison.sort(key=lambda x: float(x['Score Severidade']), reverse=True)
                    
                    # Gr√°fico visual das anomalias
                    st.plotly_chart(
                        create_anomaly_analysis_chart(anomaly_comparison),
                        use_container_width=True,
                        key="anomaly_analysis_chart"
                    )
                    
                 
        
        # Upload de arquivos - no final da sidebar
        st.sidebar.markdown("---")
        st.sidebar.subheader("üìÅ Upload de Dados Adicionais")
        st.sidebar.caption("Adicione mais arquivos CSV para an√°lise")
        
        uploaded_files = st.sidebar.file_uploader(
            "Selecione os arquivos CSV",
            type=['csv'],
            accept_multiple_files=True,
            help="üìã **Formato esperado:**\n‚Ä¢ Date, Hour, Views\n‚Ä¢ Date + hour (YYYYMMDDHH), Views\n\nüí° **Dica:** Fa√ßa upload de m√∫ltiplos arquivos para comparar marcas"
        )
        
        # Se h√° arquivos enviados, tamb√©m carregar eles (com cache)
        if uploaded_files:
            uploaded_datasets = load_multiple_datasets(uploaded_files)
            datasets.update(uploaded_datasets)  # Combinar com CSVs da pasta
            st.sidebar.success(f"‚úÖ {len(uploaded_files)} arquivo(s) adicionado(s) com sucesso!")
            st.rerun()  # Recarregar para mostrar as novas marcas
    else:
        # Mostrar mensagem quando n√£o h√° dados v√°lidos
        st.error("‚ùå Nenhum arquivo v√°lido foi carregado. Verifique o formato dos dados.")
        st.info("üí° **Dica:** Os arquivos devem conter colunas: 'Date + hour (YYYYMMDDHH)' e 'Views' OU 'Date', 'Hour' e 'Views'")

if __name__ == "__main__":
    # Verificar senha antes de mostrar o dashboard
    if check_password():
        main()
    else:
        st.stop()  # Para a execu√ß√£o se a senha estiver incorreta
